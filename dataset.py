import torch
import os
import json
from tqdm import tqdm
from torch.utils.data import Dataset
import random
from typing import List, Union, Tuple
from constants import LAYER_MAP



HIDDEN_STATES_DIR = '/share/data_instruct_sep/get_activations/mistral'


class ActivationsDatasetDynamic(Dataset):
    """
    A dataset class designed to dynamically load and return slices of neural network activations
    stored in files.

    The class is initialized with a list of file paths where activations are stored, a root directory
    for these files, and num_layers, which is either "int", in this case it would return the last n layers,
    or a tuple, in this case it would return the range of layers from start to end (inclusive).

    Attributes:
        root_dir (str): The directory containing activation files.
        dataset_files (List[str]): A list of filenames (strings) within root_dir that contain the activations to be loaded.
        activations (Tensor): The loaded activations concatenated across all specified files.
        num_layers (int or Tuple): which activation layers to load

    Methods:
        load_activations(num_layers): Loads the specified activation layers from each file in dataset_files.
        __len__(): Returns the number of samples in the dataset, determined by the second dimension of the activations tensor.
        __getitem__(idx): Returns the activations for a given index, supporting the retrieval of primary, clean, and poisoned data slices.
    """

    def __init__(self, dataset_files: List[str], root_dir: str, num_layers: Union[int, Tuple[int, int]]):
        """
        Initializes the dataset object with file paths, root directory, and the number of layers to load.

        Parameters:
            dataset_files (List[str]): List of paths to the saved activation files.
            root_dir (str): Directory containing the activation files.
            num_layers (int or Tuple): activation layers to load.
        """
        self.root_dir = root_dir
        self.dataset_files = dataset_files
        self.activations = self.load_activations(num_layers)

    def load_activations(self, num_layers: Union[int, Tuple[int, int]]):
        """
        Loads neural network activations from stored files, optionally selecting only a specified
        number of layers from each activation tensor.

        Parameters:
            num_layers (int or tuple): Specifies which layers to load from each file.

        Returns:
            torch.Tensor: A single tensor containing all requested activations concatenated along the
                        first dimension.

        Note:
            The method assumes that the activation files are stored in a consistent format, where the
            first dimension corresponds to different data types (e.g., primary, clean, poisoned), the
            second dimension indexes individual examples, the third dimension corresponds to the
            network's layers, and the fourth dimension contains the activation values from each layer.
            """
        activations = []

        for file_ in tqdm(self.dataset_files):
            file_name = os.path.join(self.root_dir, file_)

            if not os.path.exists(file_name):
                continue

            # Load the activation tensor from the current file.
            activation_file = torch.load(file_name)

            # Slice the activation tensor to select only the requested layers.
            # The slice [0:3, :, num_layers:, :] means:
            # - Select all data types (0:3),
            # - Select all examples (:),
            # - Select the last `num_layers` layers from each example (-num_layers:),
            # - Include all activation values from the selected layers (:).

            if isinstance(num_layers, int):
                activation = activation_file[0:3, :, -LAYER_MAP[num_layers]:, :]

            elif isinstance(num_layers, tuple):
                activation = activation_file[0:3, :, LAYER_MAP[num_layers[0]]: LAYER_MAP[num_layers[1]] + 1, :]

            # Append the sliced activation tensor to the list.
            activations.append(activation)

        if len(activations) == 0:
            return torch.empty(3, 0, 0, 0)

        # Concatenate all activation tensors in the list along the first dimension.
        return torch.cat(activations, dim=1)

    def __len__(self):
        """
        Returns the number of samples in the dataset.

        Returns:
            The size of the second dimension of the activations tensor, representing the number of samples.
        """
        return self.activations.size(1)

    def __getitem__(self, idx):
        """
        Retrieves the activations for a given index, returning primary, clean, and poisoned slices.

        Parameters:
            idx (int or tensor): Index of the sample to retrieve.

        Returns:
            A tuple containing the primary, clean, and poisoned activation tensors for the specified index.
        """
        if torch.is_tensor(idx):
            idx = idx.tolist()

        primary = self.activations[0, idx, :]
        clean = self.activations[1, idx, :]
        poisoned = self.activations[2, idx, :]

        return primary, clean, poisoned


class ActivationsDatasetDynamicPrimaryText(Dataset):
    """
    This class is similar to previous ones. However, it assumes that the data is stored as pairs of (primary, text) instead of triplets of (primary, clean, poisoned)

    This is mainly for testing on validation and test data.
    """

    def __init__(
            self,
            dataset_files: List[str],
            num_layers: Union[int, Tuple[int, int]],
            root_dir: str = HIDDEN_STATES_DIR,
    ):
        """
        Arguments:
            dataset_files (list): list of files of saved injection pt file
            root_dir (string): Directory with all the images.
        """
        self.root_dir = root_dir
        self.dataset_files = dataset_files
        self.num_layers = num_layers
        self.activations = self.load_activations()
        return

    def load_activations(self):
        activations = []
        for file_ in self.dataset_files:
            curr_file = torch.load(os.path.join(self.root_dir, file_))

            if isinstance(self.num_layers, int):
                activation = curr_file[:, :, -LAYER_MAP[self.num_layers]:, :]

            elif isinstance(self.num_layers, tuple):
                activation = curr_file[:, :, LAYER_MAP[self.num_layers[0]]: LAYER_MAP[self.num_layers[1]] + 1, :]

            activations.append(activation)

        return torch.cat(activations, dim=1)

    def __len__(self):
        return self.activations.size(1)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        activations_primary = self.activations[0, idx, :]
        activations_primary_with_text = self.activations[1, idx, :]

        return activations_primary, activations_primary_with_text


"""------------------------------------------------------------------------------------------------------------------------------"""


class AdversarialActivationsDatasetDynamic(Dataset):

    def __init__(self, dataset_files: List[str], adv_dataset_files: List[str], root_dir: str, root_dir_adv: str,
                 num_layers: Union[int, Tuple[int, int]]):

        self.root_dir = root_dir
        self.dataset_files = dataset_files
        self.adv_dataset_files = adv_dataset_files
        self.root_dir_adv = root_dir_adv
        self.activations = self.load_activations(num_layers)

    def load_activations(self, num_layers: Union[int, Tuple[int, int]]):
        activations = []

        for file_, adv_file in tqdm(zip(self.dataset_files, self.adv_dataset_files)):
            file_name = os.path.join(self.root_dir, file_)
            adv_file_name = os.path.join(self.root_dir_adv, adv_file)

            if not os.path.exists(file_name) or not os.path.exists(adv_file_name):
                continue

            # Load the activation tensor from the current file.

            activation_tensor = torch.load(file_name)
            adv_activation_tensor = torch.load(adv_file_name)

            activation_file = torch.cat([activation_tensor, adv_activation_tensor], dim=0)

            if isinstance(num_layers, int):
                activation = activation_file[0:4, :, -LAYER_MAP[num_layers]:, :]

            elif isinstance(num_layers, tuple):
                activation = activation_file[0:4, :, LAYER_MAP[num_layers[0]]: LAYER_MAP[num_layers[1]] + 1, :]

            # Append the sliced activation tensor to the list.
            activations.append(activation)

        if len(activations) == 0:
            return torch.empty(4, 0, 0, 0)

        # Concatenate all activation tensors in the list along the first dimension.
        return torch.cat(activations, dim=1)

    def __len__(self):
        """
        Returns the number of samples in the dataset.

        Returns:
            The size of the second dimension of the activations tensor, representing the number of samples.
        """
        return self.activations.size(1)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        primary = self.activations[0, idx, :]
        clean = self.activations[1, idx, :]
        poisoned = self.activations[2, idx, :]
        adv_poisoned = self.activations[3, idx, :]

        return primary, clean, poisoned, adv_poisoned


class AdversarialActivationsDatasetDynamicPrimaryText(Dataset):
    """
    This class is similar to previous ones. However, it assumes that the data is stored as pairs of (primary, text) instead of triplets of (primary, clean, poisoned)

    This is mainly for testing on validation and test data.
    """

    def __init__(self, dataset_files: List[str], adv_dataset_files: List[str], num_layers: Union[int, Tuple[int, int]],
                 root_dir: str = HIDDEN_STATES_DIR, root_dir_adv: str = HIDDEN_STATES_DIR):
        """
        Arguments:
            dataset_files (list): list of files of saved injection pt file
            root_dir (string): Directory with all the images.
        """
        self.root_dir = root_dir
        self.dataset_files = dataset_files
        self.num_layers = num_layers
        self.adv_dataset_files = adv_dataset_files
        self.root_dir_adv = root_dir_adv

        self.activations = self.load_activations()
        return

    def load_activations(self):
        activations = []
        for file_, adv_file in zip(self.dataset_files, self.adv_dataset_files):
            curr_file = torch.load(os.path.join(self.root_dir, file_))
            curr_file_adv = torch.load(os.path.join(self.root_dir_adv, adv_file))

            activation_file = torch.cat([curr_file, curr_file_adv], dim=0)

            if isinstance(self.num_layers, int):
                activation = activation_file[:, :, -LAYER_MAP[self.num_layers]:, :]

            elif isinstance(self.num_layers, tuple):
                activation = activation_file[:, :, LAYER_MAP[self.num_layers[0]]: LAYER_MAP[self.num_layers[1]] + 1, :]

            activations.append(activation)

        return torch.cat(activations, dim=1)

    def __len__(self):
        return self.activations.size(1)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        activations_primary = self.activations[0, idx, :]
        activations_primary_with_text = self.activations[1, idx, :]
        adv_activations_primary_with_text = self.activations[2, idx, :]

        return activations_primary, activations_primary_with_text, adv_activations_primary_with_text

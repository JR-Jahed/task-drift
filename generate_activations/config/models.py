import os
from typing import Dict

import torch
from generate_activations.models.model import Model
from constants import PROJECT_ROOT



# Set the cache directory for Hugging Face transformers
cache_dir = f"{PROJECT_ROOT}/loaded_models"
os.environ["TRANSFORMERS_CACHE"] = cache_dir
os.environ["HF_HOME"] = cache_dir

# Directory where model activation data will be stored
activation_parent_dir = "/home/40456997@eeecs.qub.ac.uk/Adversarial Activation"

# Directory where the dataset text files are stored
text_dataset_parent_dir = "/home/40456997@eeecs.qub.ac.uk/Text Dataset"

# Paths to dataset files
data = {
    "train": os.path.join(text_dataset_parent_dir, "train_subset.json"),
    # "val_clean": os.path.join(text_dataset_parent_dir, "dataset_out_clean.json"),
    "val_poisoned": os.path.join(
        text_dataset_parent_dir, "dataset_out_poisoned_v1.json"
    ),
    # "test_clean": os.path.join(text_dataset_parent_dir, "dataset_out_clean_v2.json"),
    # "test_poisoned": os.path.join(
    #     text_dataset_parent_dir, "dataset_out_poisoned_v2.json"
    # ),
}

suffix_list_dir = os.path.join(PROJECT_ROOT, "generate_activations/data")

# Initialize models with specific configurations

llama_3_8B = Model(
    name="meta-llama/Meta-Llama-3-8B-Instruct",
    output_dir=os.path.join(activation_parent_dir, "llama3_8b"),
    data=data,
    suffix_list={
        "train": os.path.join(suffix_list_dir, "llama_3_8b_train_suffix_list.json"),
        "val_poisoned": os.path.join(suffix_list_dir, "llama_3_8b_val_suffix_list.json"),
    },
    subset="train",
    torch_dtype=torch.float32,
)

phi3 = Model(
    name="microsoft/Phi-3-mini-4k-instruct",
    output_dir=os.path.join(activation_parent_dir, "phi3"),
    data=data,
    suffix_list={
        "train": os.path.join(suffix_list_dir, "phi3_train_suffix_list.json"),
        "val_poisoned": os.path.join(suffix_list_dir, "phi3_val_suffix_list.json"),
    },
    subset="train",
    torch_dtype=torch.bfloat16,
)

# Dictionary of models for easy access
models: Dict[str, Model] = {
    "llama3_8b": llama_3_8B,
    "phi3": phi3,
}
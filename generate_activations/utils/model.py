import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig


def load_model_and_tokenizer(model_path, torch_dtype=torch.bfloat16, device='cuda:0'):
    config = AutoConfig.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True, device_map='auto')
    model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch_dtype, local_files_only=True, device_map='auto').eval()

    return model, tokenizer

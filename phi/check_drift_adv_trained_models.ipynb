{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:18.596313Z",
     "start_time": "2025-08-22T08:20:15.056299Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from phi.opt_utils import get_test_prompts, get_last_token_activations_single, load_model_and_tokenizer\n",
    "from utils.data import format_prompts\n",
    "from utils.load_file_paths import load_file_paths\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "from constants import PROJECT_ROOT, LAYER_MAP\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=10000)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=100000, threshold=float('inf'))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40456997@eeecs.qub.ac.uk/PycharmProjects/TaskDriftTest/venv/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:18.609814Z",
     "start_time": "2025-08-22T08:20:18.606871Z"
    }
   },
   "cell_type": "code",
   "source": "model_name = 'phi3'",
   "id": "fb60c2377157b4f2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:18.718105Z",
     "start_time": "2025-08-22T08:20:18.715883Z"
    }
   },
   "cell_type": "code",
   "source": "model_path = f'{PROJECT_ROOT}/loaded_models/{model_name}'",
   "id": "aa210edd30be9fdf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:18.766134Z",
     "start_time": "2025-08-22T08:20:18.763406Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "c7c43a63b8889ff8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:21.916994Z",
     "start_time": "2025-08-22T08:20:18.817976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_path, torch_dtype=torch.float32 if model_name == 'llama3_8b' else torch.bfloat16)\n",
    "\n",
    "print(model.dtype)\n",
    "\n",
    "device = model.get_input_embeddings().weight.device"
   ],
   "id": "29f6ed6a5e6ef365",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34a764f27e764bee84aab60858e497b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:22.652692Z",
     "start_time": "2025-08-22T08:20:21.942683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_prompts = get_test_prompts()\n",
    "print(len(test_prompts))"
   ],
   "id": "9620f32a9105b7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31134\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:22.702881Z",
     "start_time": "2025-08-22T08:20:22.698734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).squeeze(-1)\n"
   ],
   "id": "bd9ecd846dd0e08e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:22.778265Z",
     "start_time": "2025-08-22T08:20:22.768076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear_models = {}\n",
    "\n",
    "layers = []\n",
    "\n",
    "model_type = 'suffix'  #  Either pgd or suffix\n",
    "\n",
    "for (dirpath, dir_names, filenames) in os.walk(f'{PROJECT_ROOT}/adv_trained_linear_probes_{model_type}/{model_name}'):\n",
    "    layers = [int(dir_name) for dir_name in dir_names]\n",
    "    break\n",
    "\n",
    "layers.sort()\n",
    "\n",
    "for i in layers:\n",
    "    linear_models[i] = LogisticRegression(input_dim=3072)\n",
    "    linear_models[i].load_state_dict(torch.load(os.path.join(PROJECT_ROOT, f'adv_trained_linear_probes_{model_type}', model_name, str(i), 'model.pt')))\n",
    "    linear_models[i].eval()\n"
   ],
   "id": "a502f4bcdc411ca3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:22.841398Z",
     "start_time": "2025-08-22T08:20:22.837428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Store these to avoid loading the activation files repeatedly, which is an expensive operation\n",
    "\n",
    "previous_activation_filepath = ''\n",
    "activations = None\n",
    "\n",
    "def get_primary_activation(index, model, layer, subset):\n",
    "\n",
    "    global previous_activation_filepath, activations\n",
    "\n",
    "    index_in_file = index - int(index / 1000) * 1000\n",
    "\n",
    "    if subset == 'test':\n",
    "        filepaths = load_file_paths(f'{PROJECT_ROOT}/data_files/test_poisoned_files_{model}.txt')\n",
    "    else:\n",
    "        filepaths = load_file_paths(f'{PROJECT_ROOT}/data_files/train_files_{model}.txt')\n",
    "\n",
    "    activation_file_index_in_list = 0\n",
    "\n",
    "    for idx, filepath in enumerate(filepaths):\n",
    "        if filepath.count(f'_{int(index / 1000) * 1000}_{(int(index / 1000) + 1) * 1000}_') == 1:\n",
    "            activation_file_index_in_list = idx\n",
    "            break\n",
    "\n",
    "    current_activation_filepath = f'/home/40456997@eeecs.qub.ac.uk/Reduced Activation/{model}/{subset}/{filepaths[activation_file_index_in_list]}'\n",
    "\n",
    "    if previous_activation_filepath != current_activation_filepath:\n",
    "        activations = torch.load(current_activation_filepath)\n",
    "\n",
    "    previous_activation_filepath = current_activation_filepath\n",
    "\n",
    "    return activations[0][index_in_file][LAYER_MAP[layer]]\n"
   ],
   "id": "44d0f2a82653e5e4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:22.912271Z",
     "start_time": "2025-08-22T08:20:22.907980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_task_drift(prompt_index, hidden_states):\n",
    "\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    for num_layer, linear_model in linear_models.items():\n",
    "        primary_activation = get_primary_activation(prompt_index, model_name, num_layer, subset='test')\n",
    "        poisoned_activation = hidden_states[num_layer][:, -1].cpu()\n",
    "\n",
    "        delta = (poisoned_activation - primary_activation).to(torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = linear_model(delta)\n",
    "            prob = torch.sigmoid(logits)\n",
    "            label = (prob >= .5).long()\n",
    "\n",
    "            labels.append(label.item())\n",
    "            probs.append([1 - prob.item(), prob.item()])\n",
    "\n",
    "    return labels, probs\n"
   ],
   "id": "45f5852c86bb250e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:22.982455Z",
     "start_time": "2025-08-22T08:20:22.980253Z"
    }
   },
   "cell_type": "code",
   "source": "# adv_suffix = r\"\"\"comuna cidade comuna$\\ $\\{6 +\\frac revision Guillaume,..., Riemann\\,\\mapsto$\\textbf MRlabel converges',\"\"\"",
   "id": "7bbf38b30ec9c74e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:23.052725Z",
     "start_time": "2025-08-22T08:20:23.050509Z"
    }
   },
   "cell_type": "code",
   "source": "# print(len(tokenizer(adv_suffix).input_ids))",
   "id": "8b174823ae82b3bd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:23.122063Z",
     "start_time": "2025-08-22T08:20:23.119101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_probs(probs):\n",
    "    formatted_probs = []\n",
    "    for prob_pair in probs:\n",
    "        formatted_pair = [f\"{p:.8f}\" for p in prob_pair]\n",
    "        formatted_probs.append(f\"[{formatted_pair[0]}, {formatted_pair[1]}]\")\n",
    "    probs_str = \"[\" + \", \".join(formatted_probs) + \"]\"\n",
    "\n",
    "    return probs_str"
   ],
   "id": "eb9ab16a1bac2c0f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:20:26.689977Z",
     "start_time": "2025-08-22T08:20:23.185393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "result_path = f'{PROJECT_ROOT}/test_results/{model_name}_result_adv_trained_models.json'\n",
    "\n",
    "if os.path.exists(result_path):\n",
    "    result = json.load(open(result_path, 'r'))\n",
    "else:\n",
    "\n",
    "    result = {\n",
    "        'Result list': [\n",
    "\n",
    "        ]\n",
    "    }\n"
   ],
   "id": "f394f195bf9e83b9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "optimisation_result_path = f'{PROJECT_ROOT}/opt_results/{model_name}_optimisation_result.json'\n",
    "\n",
    "adv_suffixes = []\n",
    "\n",
    "if model_type == 'pgd':\n",
    "    if os.path.exists(optimisation_result_path):\n",
    "        optimisation_result = json.load(open(optimisation_result_path, 'r'))\n",
    "        for opt_result in optimisation_result['Result list'][len(result['Result list']):]:\n",
    "            adv_suffixes.append(opt_result['Result']['Iteration log'][-1]['suffix'])\n",
    "else:\n",
    "    test_suffixes = json.load(open(f'{PROJECT_ROOT}/phi3_test_suffix_list.json', 'r'))\n",
    "    adv_suffixes = test_suffixes['Suffix list']\n",
    "\n",
    "for adv_suf in adv_suffixes:\n",
    "    print(adv_suf)\n",
    "\n"
   ],
   "id": "66d9a7ef0b84637d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "start_prompt = 0\n",
    "total_prompts = 31134\n",
    "\n",
    "for adv_suffix in adv_suffixes:\n",
    "\n",
    "    prompt_indices = random.sample(range(total_prompts), 12000)\n",
    "\n",
    "    # Indices must be sorted to avoid loading same activation file multiple times\n",
    "    prompt_indices.sort()\n",
    "\n",
    "    result_dict = {\n",
    "        \"Suffix\": adv_suffix,\n",
    "        \"Prompt indices\": prompt_indices,\n",
    "        \"Attack result list\": [],\n",
    "        \"Total number of prompts correctly classified by a specific number of classifiers\": {\n",
    "            \"Without suffix\": {str(key): 0 for key in range(len(layers) + 1)},\n",
    "            \"With suffix\": {str(key): 0 for key in range(len(layers) + 1)}\n",
    "        },\n",
    "        \"Layerwise correct classification\": {\n",
    "            \"Without suffix\": {str(key): 0 for key in layers},\n",
    "            \"With suffix\": {str(key): 0 for key in layers}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cnt_correct_classification_without_suffix = result_dict[\"Total number of prompts correctly classified by a specific number of classifiers\"][\"Without suffix\"]\n",
    "    cnt_correct_classification_with_suffix = result_dict[\"Total number of prompts correctly classified by a specific number of classifiers\"][\"With suffix\"]\n",
    "\n",
    "    layerwise_correct_classification_without_suffix = result_dict[\"Layerwise correct classification\"][\"Without suffix\"]\n",
    "    layerwise_correct_classification_with_suffix = result_dict[\"Layerwise correct classification\"][\"With suffix\"]\n",
    "\n",
    "\n",
    "    print(f\"Adv suffix: {adv_suffix}\")\n",
    "\n",
    "    for prompt_index in prompt_indices:\n",
    "        gc.collect()\n",
    "\n",
    "        prompt_without_adv_suffix = copy.deepcopy(test_prompts[prompt_index])\n",
    "\n",
    "        primary, primary_clean, primary_poisoned = (\n",
    "            format_prompts([prompt_without_adv_suffix], True)\n",
    "        )\n",
    "\n",
    "        outputs = get_last_token_activations_single(primary_poisoned[0], tokenizer, model)\n",
    "        hidden_states = outputs['hidden_states'][1:]\n",
    "\n",
    "        labels_without_adv_suffix, probs_without_adv_suffix = check_task_drift(prompt_index, hidden_states)\n",
    "\n",
    "        cnt_correct_classification_without_suffix[str(labels_without_adv_suffix.count(1))] += 1\n",
    "\n",
    "        for i, num_layer in enumerate(layers):\n",
    "            layerwise_correct_classification_without_suffix[str(num_layer)] += labels_without_adv_suffix[i]\n",
    "\n",
    "        result_dict[\"Attack result list\"].append(\n",
    "            {\n",
    "                \"Without suffix\": {\n",
    "                    \"labels\": labels_without_adv_suffix,\n",
    "                    \"probs\": probs_without_adv_suffix,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------\n",
    "\n",
    "        prompt_with_adv_suffix = copy.deepcopy(test_prompts[prompt_index])\n",
    "\n",
    "        prompt_with_adv_suffix['final_text_paragraph'] = prompt_with_adv_suffix['final_text_paragraph'] + \" \" + adv_suffix\n",
    "\n",
    "        primary, primary_clean, primary_poisoned = (\n",
    "            format_prompts([prompt_with_adv_suffix], True)\n",
    "        )\n",
    "\n",
    "        outputs = get_last_token_activations_single(primary_poisoned[0], tokenizer, model)\n",
    "        hidden_states = outputs['hidden_states'][1:]\n",
    "\n",
    "        labels_with_adv_suffix, probs_with_adv_suffix = check_task_drift(prompt_index, hidden_states)\n",
    "\n",
    "        cnt_correct_classification_with_suffix[str(labels_with_adv_suffix.count(1))] += 1\n",
    "\n",
    "        for i, num_layer in enumerate(layers):\n",
    "            layerwise_correct_classification_with_suffix[str(num_layer)] += labels_with_adv_suffix[i]\n",
    "\n",
    "        result_dict[\"Attack result list\"][-1][\"With suffix\"] = {\n",
    "                    \"labels\": labels_with_adv_suffix,\n",
    "                    \"probs\": probs_with_adv_suffix,\n",
    "                }\n",
    "\n",
    "        print(f\"Prompt index: {prompt_index}\")\n",
    "        print(f\"Without suffix:    labels: {labels_without_adv_suffix}  probs: {format_probs(probs_without_adv_suffix)}\")\n",
    "        print(f\"With suffix:       labels: {labels_with_adv_suffix}  probs: {format_probs(probs_with_adv_suffix)}\\n\")\n",
    "\n",
    "\n",
    "    cur_time = time.time()\n",
    "    print(f\"Total elapsed time: {cur_time - start_time} seconds\\n\")\n",
    "\n",
    "    print(\"Total number of prompts correctly classified by a specific number of classifiers\")\n",
    "    print(f\"Without suffix: {cnt_correct_classification_without_suffix}\")\n",
    "    print(f\"With suffix: {cnt_correct_classification_with_suffix}\\n\")\n",
    "\n",
    "    print(f\"Layerwise correct classification without suffix: {layerwise_correct_classification_without_suffix}\")\n",
    "    print(f\"Layerwise correct classification with suffix: {layerwise_correct_classification_with_suffix}\\n\")\n",
    "\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    result['Result list'].append(result_dict)\n",
    "\n",
    "    with open(result_path, 'w') as f:\n",
    "        json.dump(result, f, indent=4)\n"
   ],
   "id": "7162e304c127cee5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

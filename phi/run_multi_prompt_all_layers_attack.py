import torch
import torch.nn as nn
import numpy as np
import time
import pickle
from phi.custom_model import CustomModel
from phi.opt_utils import get_nonascii_toks, token_gradients, sample_control, get_filtered_cands, get_logits, load_model_and_tokenizer, get_training_prompts
from phi.suffix_manager import SuffixManager
from utils.load_file_paths import load_file_paths
import random
import os
import sys
import gc
import json
from constants import PROJECT_ROOT, LAYER_MAP

np.set_printoptions(suppress=True, linewidth=10000)
torch.set_printoptions(sci_mode=False, linewidth=100000, threshold=float('inf'))


model_name = 'phi3'

model_path = f'{PROJECT_ROOT}/loaded_models/{model_name}'



def format_probs(probs):
    formatted_probs = []
    for prob_pair in probs:
        formatted_pair = [f"{p:.8f}" for p in prob_pair]
        formatted_probs.append(f"[{formatted_pair[0]}, {formatted_pair[1]}]")
    probs_str = "[" + ", ".join(formatted_probs) + "]"

    return probs_str

def format_losses(losses):
    losses_str = "[" + ", ".join([f'{loss:.8f}' for loss in losses]) + "]"
    return losses_str


def check_probabilities(probs_all_prompts_all_layers):
    mn = 100.0

    for probs_current_prompt_all_layers in probs_all_prompts_all_layers:
        for probs_current_prompt_current_layer in probs_current_prompt_all_layers:
            mn = min(mn, probs_current_prompt_current_layer[0])
    return mn


def percentage_of_successful_prompts_all_layers(probs_all_prompts_all_layers, confidence_threshold=.8):

    cnt = 0

    for probs_current_prompt_all_layers in probs_all_prompts_all_layers:
        mn = 100.0
        for probs_current_prompt_current_layer in probs_current_prompt_all_layers:
            mn = min(mn, probs_current_prompt_current_layer[0])

        if mn >= confidence_threshold:
            cnt += 1

    return cnt / len(probs_all_prompts_all_layers)


def get_primary_activation(index, model, layer, subset):

    index_in_file = index - int(index / 1000) * 1000

    if subset == 'test':
        filepaths = load_file_paths(f'{PROJECT_ROOT}/data_files/test_poisoned_files_{model}.txt')
    else:
        filepaths = load_file_paths(f'{PROJECT_ROOT}/data_files/train_files_{model}.txt')

    activation_file_index_in_list = 0

    for idx, filepath in enumerate(filepaths):
        if filepath.count(f'_{int(index / 1000) * 1000}_{(int(index / 1000) + 1) * 1000}_') == 1:
            activation_file_index_in_list = idx
            break

    activations = torch.load(f'/home/40456997@eeecs.qub.ac.uk/Reduced Activation/{model}/{subset}/{filepaths[activation_file_index_in_list]}')

    return activations[0][index_in_file][LAYER_MAP[layer]]


def get_primary_activations(prompt_index, device, layers):

    primary_activations = {
        layer: get_primary_activation(index=prompt_index, model=model_name, layer=layer, subset='training').to(device) for layer in layers
    }

    return primary_activations


def add_next_prompt(training_prompts, prompt_indices, texts, primary_activations_all_prompts, device, layers):
    # Add next prompt
    texts.append(training_prompts[prompt_indices[len(texts)]])

    primary_activations = get_primary_activations(prompt_indices[len(primary_activations_all_prompts)], device, layers)
    # Add primary activations of this prompt
    primary_activations_all_prompts.append(primary_activations)


def check_misclassification(custom_model, model, linear_models, adv_suffix, suffix_manager, primary_activations):
    device = custom_model.base_model.get_input_embeddings().weight.device
    input_ids = suffix_manager.get_input_ids(adv_suffix).unsqueeze(0)
    input_ids = input_ids.to(device)

    attention_mask = torch.ones_like(input_ids).to(device)

    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)

    labels = []
    probs = []

    for num_layer, linear_model in linear_models.items():
        poisoned_activation = outputs['hidden_states'][num_layer + 1][0, -1].float()
        delta = (poisoned_activation - primary_activations[num_layer]).detach().cpu().float().numpy().reshape(1, -1)

        label = linear_model.predict(delta)
        prob = linear_model.predict_proba(delta)

        labels.append(label[0].tolist())
        probs.append(prob[0].tolist())

    return labels, probs



def run_attack(
        tokenizer,
        model,
        custom_model,
        linear_models,
        training_prompts,
        prompt_indices,
        texts,
        adv_string_init,
        allow_non_ascii,
        num_steps,
        primary_activations_all_prompts,
        device,
        layers,
        batch_size,
        topk,
        target
):
    suffix_manager = SuffixManager(tokenizer, texts[0], adv_string_init)
    not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer)
    adv_suffix = adv_string_init

    start_time = time.time()

    last_added = 0

    result_dict = {
        'Iteration log': [],
        'Total time': 0.0
    }

    for i in range(num_steps + 1):

        gc.collect()

        coordinate_grad_all_prompts = None
        losses_all_prompts_all_layers = []

        input_ids = None

        # A list of lists
        labels_all_prompts_all_layers = []

        # A list of lists of lists
        probs_all_prompts_all_layers = []

        original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

        for text, primary_activations_current_prompt in zip(texts, primary_activations_all_prompts):

            # Step 1. Encode user prompt (behavior + adv suffix) as tokens and return token ids.
            suffix_manager = SuffixManager(tokenizer, text, adv_string_init)
            input_ids = suffix_manager.get_input_ids(adv_suffix)
            input_ids = input_ids.to(device)

            # Step 2. Compute Coordinate Gradient
            coordinate_grad_current_prompt, losses_current_prompt_all_layers = token_gradients(
                custom_model,
                input_ids,
                suffix_manager.adv_string_slice,
                target,
                primary_activations=primary_activations_current_prompt
            )

            losses_all_prompts_all_layers.append(losses_current_prompt_all_layers)

            if coordinate_grad_all_prompts is None:
                coordinate_grad_all_prompts = coordinate_grad_current_prompt  # shape: (num_tokens, vocab_size)
            else:
                coordinate_grad_all_prompts += coordinate_grad_current_prompt

            labels_current_prompt_all_layers, probs_current_prompt_all_layers = check_misclassification(
                custom_model,
                model,
                linear_models,
                adv_suffix,
                suffix_manager,
                primary_activations_current_prompt
            )
            labels_all_prompts_all_layers.append(labels_current_prompt_all_layers)
            probs_all_prompts_all_layers.append(probs_current_prompt_all_layers)

            gc.collect()

        sys.stdout = original_stdout

        print(f"i: {i}")

        for idx in range(len(labels_all_prompts_all_layers)):
            print(f"losses: {format_losses(losses_all_prompts_all_layers[idx])} labels: {labels_all_prompts_all_layers[idx]} probs: {format_probs(probs_all_prompts_all_layers[idx])}")
        print(adv_suffix)
        print("-------------------------------------------------------\n")

        result_dict['Iteration log'].append(
            {
                'losses': losses_all_prompts_all_layers,
                'labels': labels_all_prompts_all_layers,
                'probs': probs_all_prompts_all_layers,
                'suffix': adv_suffix
            }
        )

        elapsed_time = time.time() - start_time

        # The aim is to find a suffix optimised on a diverse range of prompts
        # It's about finding a balance between the amount of time spent on optimising one suffix
        # and optimising the suffix on many prompts.

        # Either the suffix has been optimised on 100 or more prompts
        # Or, 6 hours have passed
        # Or, 5 hours have passed and the suffix has been optimised on 70 or more prompts
        # Or, more hours have passed than number of prompts if there are more than one prompt
        # Or, two hours have passed or 200 iterations have been completed, but couldn't find a suffix for even one prompt

        if (i == num_steps or len(texts) >= 100 or elapsed_time >= 6 * 60 * 60
                or (elapsed_time >= 5 * 60 * 60 and len(texts) >= 70) or (len(texts) > 1 and elapsed_time >= len(texts) * 60 * 60)
                or (len(texts) == 1 and (elapsed_time >= 2 * 60 * 60 or i >= 200))):
            break

        # Step 3. Sample a batch of new tokens based on the coordinate gradient.
        # Notice that we only need the one that minimizes the loss.
        with torch.no_grad():

            # Step 3.1 Slice the input to locate the adversarial suffix.
            adv_suffix_tokens = input_ids[suffix_manager.adv_string_slice].to(device)

            # Step 3.2 Randomly sample a batch of replacements.
            # Encoded suffixes
            new_adv_suffix_tokens = sample_control(
                adv_suffix_tokens,
                coordinate_grad_all_prompts,
                batch_size,
                not_allowed_tokens=not_allowed_tokens,
                topk=topk
            )

            # Step 3.3 This step ensures all adversarial candidates have the same number of tokens.
            # Decoded suffixes
            new_adv_suffix = get_filtered_cands(
                tokenizer,
                new_adv_suffix_tokens,
                filter_cand=True,
                curr_control=adv_suffix
            )

            losses_all_prompts_all_layers = None

            for text, primary_activations_current_prompt in zip(texts, primary_activations_all_prompts):

                suffix_manager = SuffixManager(tokenizer, text, adv_string_init)
                input_ids = suffix_manager.get_input_ids(adv_suffix)
                input_ids = input_ids.to(device)

                # Step 3.4 Compute loss on these candidates and take the argmin.
                logits_per_classifier = get_logits(
                    custom_model=custom_model,
                    tokenizer=tokenizer,
                    input_ids=input_ids,
                    control_slice=suffix_manager.adv_string_slice,
                    primary_activations=primary_activations_current_prompt,
                    test_controls=new_adv_suffix,
                    batch_size=8  # decrease this number if you run into OOM.
                )
                last_classifier_logits = next(reversed(logits_per_classifier.values()))
                target = target.to(last_classifier_logits.dtype)
                expanded_target = target.expand_as(last_classifier_logits)

                losses_current_prompt_all_layers = None

                for num_layer, logits_current_layer in logits_per_classifier.items():
                    losses_current_layer = nn.BCEWithLogitsLoss(reduction='none')(logits_current_layer, expanded_target)

                    if losses_current_prompt_all_layers is None:
                        losses_current_prompt_all_layers = losses_current_layer
                    else:
                        losses_current_prompt_all_layers += losses_current_layer
                    del logits_current_layer

                if losses_all_prompts_all_layers is None:
                    losses_all_prompts_all_layers = losses_current_prompt_all_layers  # shape: (batch_size, 1)
                else:
                    losses_all_prompts_all_layers += losses_current_prompt_all_layers
                gc.collect()
                torch.cuda.empty_cache()

            # Average across prompts
            losses_current_prompt_all_layers /= len(texts)

            best_new_adv_suffix_id = losses_all_prompts_all_layers.argmin()
            best_new_adv_suffix = new_adv_suffix[best_new_adv_suffix_id]

            adv_suffix = best_new_adv_suffix

        if len(texts) < len(prompt_indices):

            if percentage_of_successful_prompts_all_layers(probs_all_prompts_all_layers, confidence_threshold=.7) >= .8:
                # If the attack is successful on 80% or more prompts, add next prompt(s)

                if len(texts) >= 10:
                    # If the suffix has been optimised for 10 or more prompts, it has hopefully reached
                    # a stable point where it will work for many prompts. Therefore, it might be sagacious
                    # to add 1/3 new prompts to the list to save time
                    for j in range(int(len(texts) / 3)):
                        if len(texts) < len(prompt_indices):
                            add_next_prompt(training_prompts, prompt_indices, texts, primary_activations_all_prompts, device, layers)
                else:
                    add_next_prompt(training_prompts, prompt_indices, texts, primary_activations_all_prompts, device, layers)
                    last_added = i

            if len(texts) >= 5 and i - last_added == 10:
                # If there are already 5 or more prompts and the latest one or any previous one is being recalcitrant
                # and the suffix is not working on it, it might be helpful to add a new prompt
                add_next_prompt(training_prompts, prompt_indices, texts, primary_activations_all_prompts, device, layers)
                last_added = i
            elif len(texts) >= 3 and i - last_added == 20:
                add_next_prompt(training_prompts, prompt_indices, texts, primary_activations_all_prompts, device, layers)
                last_added = i
            elif len(texts) == 2 and i - last_added == 30:
                add_next_prompt(training_prompts, prompt_indices, texts, primary_activations_all_prompts, device, layers)
                last_added = i


    end_time = time.time()
    print(f"Total time: {end_time - start_time} seconds")

    result_dict['Total time'] = end_time - start_time

    return result_dict


if __name__ == "__main__":

    adv_string_init_list = [
        r"""$ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $""",
        r"""* * * * * * * * * * * * * * * * * * * *""",
        r"""& @ ^ # | & @ ^ # | & @ ^ # | & @ ^ # |""",
        r"""| < { ] > [ } + % ? . ) / ~ ( , - = _ @""",
        r"""& & : : $ $ ; ; _ _ # # ! ! + + ? ? = =""",
        r"""# # # # # & & & & & @ @ @ @ @ ? ? ? ? ?""",
        r"""* ) # ~ / | > } ! @ [ = _ ( { : ^ ) ] -""",
        r"""yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes""",
        r"""no no no no no no no no no no no no no no no no no no no no""",
        r"""ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok""",
        r"""hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi""",

        r"""% % % % % % % % % % % % % % % % % % % %""",
        r"""+ + + + + + + + + + + + + + + + + + + +""",
        r"""- - - - - - - - - - - - - - - - - - - -""",
        r"""= = = = = = = = = = = = = = = = = = = =""",
        r"""@ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @""",
        r"""O @ ! x @ & @ R $ B @ P @ Q @ T } W @ M""",
        r"""@ ( t _ z | @ ; y [ v @ $ l j * i @ : r""",

        r"""# hi @ no @ ! ! ~ < n { s ' ; @ u > c .""",  # with unicode
        r"""* $ % ^ + & $ ! $ ! [ # ! * $ ! % ] - |""",  # with unicode
        r"""$ . ! > { ! } ) ! $ ( / @ $ ^ ! = : & !""",  # with unicode
        r"""! ! j $ $ a * * h = = e & & d < < > > #""",  # with unicode
        r"""@ - = @ _ ! $ ^ $ ! ; < % | ! & $ ^ # =""",  # with unicode
        r"""t * a - s % k ! $ d : r ^ i | f = t @ ;"""   # with unicode
    ]

    model, tokenizer = load_model_and_tokenizer(model_path)
    print(model.dtype)

    training_prompts = get_training_prompts()
    num_training_prompts = len(training_prompts)

    num_steps = 500
    topk = 256
    batch_size = 512


    allow_non_ascii = True  # you can set this to True to use unicode tokens


    linear_models = {}

    layers = []

    for (dirpath, dir_names, filenames) in os.walk(f'{PROJECT_ROOT}/trained_linear_probes_microsoft/{model_name}'):
        layers = [int(dir_name) for dir_name in dir_names]
        break

    layers.sort()

    for i in layers:
        linear_models[i] = pickle.load(open(f'{PROJECT_ROOT}/trained_linear_probes_microsoft/{model_name}/{i}/model.pickle', 'rb'))

    custom_model = CustomModel(model, linear_models)
    device = custom_model.base_model.get_input_embeddings().weight.device

    target = torch.tensor([0], dtype=torch.float32)
    target = target.to(device)

    result_path = f'{PROJECT_ROOT}/opt_results/{model_name}_optimisation_result.json'

    start_idx = 0

    if os.path.exists(result_path):
        optimisation_result = json.load(open(result_path, 'r'))
        start_idx = len(optimisation_result['Result list'])
    else:
        optimisation_result = {
            "Result list": []
        }

    for idx in range(start_idx, len(adv_string_init_list)):
        print(f"Suffix idx: {idx}")

        adv_string_init = adv_string_init_list[idx]

        # Randomly select n from the training prompts
        prompt_indices = random.sample(range(num_training_prompts), 200)

        # Get the text from the train dataset
        texts = [training_prompts[prompt_indices[0]]]

        # A list of dict
        primary_activations_all_prompts = [get_primary_activations(prompt_indices[0], device, layers)]

        optimisation_result['Result list'].append(
            {
                "Initial suffix": adv_string_init,
                "Prompt indices": prompt_indices,
                "Result": {}
            }
        )

        result_dict = run_attack(
            tokenizer=tokenizer,
            model=model,
            custom_model=custom_model,
            linear_models=linear_models,
            training_prompts=training_prompts,
            prompt_indices=prompt_indices,
            texts=texts,
            adv_string_init=adv_string_init,
            allow_non_ascii=allow_non_ascii,
            num_steps=num_steps,
            primary_activations_all_prompts=primary_activations_all_prompts,
            device=device,
            layers=layers,
            batch_size=batch_size,
            topk=topk,
            target=target,
        )

        optimisation_result['Result list'][-1]['Result'] = result_dict

        with open(result_path, 'w') as f:
            json.dump(optimisation_result, f, indent=4)




# def run_phi_attack_from_another_file():
#     adv_string_init_list = [
#         r"""$ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $""",
#         r"""* * * * * * * * * * * * * * * * * * * *""",
#         r"""& @ ^ # | & @ ^ # | & @ ^ # | & @ ^ # |""",
#         r"""| < { ] > [ } + % ? . ) / ~ ( , - = _ @""",
#         r"""& & : : $ $ ; ; _ _ # # ! ! + + ? ? = =""",
#         r"""# # # # # & & & & & @ @ @ @ @ ? ? ? ? ?""",
#         r"""* ) # ~ / | > } ! @ [ = _ ( { : ^ ) ] -""",
#         r"""yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes""",
#         r"""no no no no no no no no no no no no no no no no no no no no""",
#         r"""ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok ok""",
#         r"""hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi""",
#
#         r"""% % % % % % % % % % % % % % % % % % % %""",
#         r"""+ + + + + + + + + + + + + + + + + + + +""",
#         r"""- - - - - - - - - - - - - - - - - - - -""",
#         r"""= = = = = = = = = = = = = = = = = = = =""",
#         r"""@ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @""",
#         r"""O @ ! x @ & @ R $ B @ P @ Q @ T } W @ M""",
#         r"""@ ( t _ z | @ ; y [ v @ $ l j * i @ : r""",
#         r"""# hi @ no @ ! ! ~ < n { s ' ; @ u > c .""",
#     ]
#
#     model, tokenizer = load_model_and_tokenizer(model_path)
#     print(model.dtype)
#
#     training_prompts = get_training_prompts()
#
#     num_steps = 500
#     topk = 256
#     batch_size = 512
#
#     allow_non_ascii = False  # you can set this to True to use unicode tokens
#
#     linear_models = {}
#
#     layers = []
#
#     for (dirpath, dir_names, filenames) in os.walk(f'{PROJECT_ROOT}/trained_linear_probes_microsoft/{model_name}'):
#         layers = [int(dir_name) for dir_name in dir_names]
#         break
#
#     layers.sort()
#
#     for i in layers:
#         linear_models[i] = pickle.load(
#             open(f'{PROJECT_ROOT}/trained_linear_probes_microsoft/{model_name}/{i}/model.pickle', 'rb'))
#
#     custom_model = CustomModel(model, linear_models)
#     device = custom_model.base_model.get_input_embeddings().weight.device
#
#     target = torch.tensor([0], dtype=torch.float32)
#     target = target.to(device)
#
#     result_path = f'{PROJECT_ROOT}/opt_results/{model_name}_optimisation_result.json'
#
#     start_idx = 0
#     if os.path.exists(result_path):
#         optimisation_result = json.load(open(result_path, 'r'))
#         start_idx = len(optimisation_result['Result list'])
#     else:
#         optimisation_result = {
#             "Result list": []
#         }
#
#     print(f"phi start idx: {start_idx}")
#
#     for idx in range(start_idx, len(adv_string_init_list)):
#
#         print(f"Suffix idx: {idx}")
#
#         adv_string_init = adv_string_init_list[idx]
#
#         # Randomly select 50 from the first 100000 prompts
#         prompt_indices = random.sample(range(100000), 50)
#
#         # Get the text from the train dataset
#         texts = [training_prompts[prompt_indices[0]]]
#
#         # A list of dict
#         primary_activations_all_prompts = [get_primary_activations(prompt_indices[0], device, layers)]
#
#         optimisation_result['Result list'].append(
#             {
#                 "Initial suffix": adv_string_init,
#                 "Prompt indices": prompt_indices,
#                 "Result": {}
#             }
#         )
#
#         result_dict = run_attack(
#             tokenizer=tokenizer,
#             model=model,
#             custom_model=custom_model,
#             linear_models=linear_models,
#             training_prompts=training_prompts,
#             prompt_indices=prompt_indices,
#             texts=texts,
#             adv_string_init=adv_string_init,
#             allow_non_ascii=allow_non_ascii,
#             num_steps=num_steps,
#             primary_activations_all_prompts=primary_activations_all_prompts,
#             device=device,
#             layers=layers,
#             batch_size=batch_size,
#             topk=topk,
#             target=target,
#         )
#
#         optimisation_result['Result list'][-1]['Result'] = result_dict
#
#         with open(result_path, 'w') as f:
#             json.dump(optimisation_result, f, indent=4)
#
#
#     del model, tokenizer
#     gc.collect()
#     torch.cuda.empty_cache()



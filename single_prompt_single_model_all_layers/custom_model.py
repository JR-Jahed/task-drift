import torch
import torch.nn as nn


class CustomModel(nn.Module):
    def __init__(self, base_model, linear_models_dict):

        """

        :param base_model: The LLM
        :param linear_models_dict: A dict of linear models where keys correspond to nth layer and values correspond to linear classifiers
        """

        super(CustomModel, self).__init__()
        self.base_model = base_model

        self.linear_layers = nn.ModuleDict()

        for num_layer, linear_model in linear_models_dict.items():
            W = linear_model.coef_
            b = linear_model.intercept_
            self.linear_layers[str(num_layer)] = nn.Linear(in_features=W.shape[1], out_features=1)

            with torch.no_grad():
                self.linear_layers[num_layer].weight.copy_(torch.tensor(W, dtype=torch.float32))
                self.linear_layers[num_layer].bias.copy_(torch.tensor(b, dtype=torch.float32))
                self.linear_layers[num_layer].weight.requires_grad = False
                self.linear_layers[num_layer].bias.requires_grad = False

        # Move linear layer to base model's embedding device
        target_device = base_model.get_input_embeddings().weight.device
        self.linear_layers = self.linear_layers.to(target_device)

    def forward(self, primary_activations, input_ids=None, inputs_embeds=None, attention_mask=None):

        """

        :param primary_activations: A dict where keys correspond to nth layer and values correspond to primary activation
        :return:
        """

        if input_ids is None and inputs_embeds is None:
            raise ValueError("Either input_ids or input_embeds should be provided")

        if input_ids is not None:
            outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
        else:
            outputs = self.base_model(inputs_embeds=inputs_embeds, output_hidden_states=True)

        logits = {}
        # hidden_states[0] is embedding, hidden_states[1] is layer 0, ..., hidden_states[32] is layer 31
        hidden_states = outputs['hidden_states'][1:]  # Exclude embedding layer, now 0-indexed

        for num_layer_str, linear_layer in self.linear_layers.items():
            num_layer = int(num_layer_str)

            # Get the last token activation from the corresponding layer
            last_token_activation = hidden_states[num_layer][:, -1]  # Now correctly 0-indexed
            delta = (last_token_activation - primary_activations[num_layer]).float()

            logits[num_layer] = linear_layer(delta)

        return logits, hidden_states

import torch
import torch.nn as nn


@torch.no_grad()
def clamp_linf(x, x0, eps):
    # Project x back to L-inf ball around x0 of radius eps
    return torch.max(torch.min(x, x0 + eps), x0 - eps)


def pgd_torch_linear(model, primary, poisoned, epsilon=0.1, alpha=0.01, steps=20, target_label=0):
    """
    Targeted PGD that perturbs the poisoned activation so the linear model predicts target_label (default: 0, 'clean').
    Assumes:
      - primary, poisoned: 1D tensors of shape [D] on same device/dtype as model
      - model(x) returns logits of shape [1] for binary classification
    Returns:
      adv_poisoned (1D tensor [D])
    """
    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype

    primary = primary.to(device=device, dtype=dtype)
    x0 = poisoned.to(device=device, dtype=dtype)  # start from the original poisoned activation
    x = x0.clone().detach().requires_grad_(True)

    y_t = torch.tensor(float(target_label), device=device, dtype=dtype)
    criterion = nn.BCEWithLogitsLoss()

    for _ in range(steps):
        # loss on delta = (x - primary)
        delta = (x - primary)  # shape [D]
        logits = model(delta.unsqueeze(0)).squeeze(0)  # [1] -> scalar
        loss = criterion(logits, y_t)

        if x.grad is not None:
            x.grad.zero_()
        loss.backward()

        # targeted toward label 0 => minimize loss wrt target 0 => descend
        with torch.no_grad():
            x = x - alpha * torch.sign(x.grad)
            x = clamp_linf(x, x0, epsilon)
        x.requires_grad_(True)

    return x.detach()  # adv_poisoned (1D)



@torch.no_grad()
def clamp_linf_(x, x0, eps):
    # In-place projection onto Lâˆž ball of radius eps around x0
    x.copy_(torch.max(torch.min(x, x0 + eps), x0 - eps))


def pgd_batch_torch_linear(
        model,
        primary,
        poisoned,
        epsilon=0.1,
        alpha=0.01,
        steps=20,
        target_label=0,
):
    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype
    primary = primary.to(device=device, dtype=dtype)

    x0 = poisoned.to(device=device, dtype=dtype).detach()
    x = x0.clone().detach().requires_grad_(True)
    y_t = torch.full((primary.size(0),), float(target_label), device=device, dtype=dtype)
    criterion = nn.BCEWithLogitsLoss()

    for step in range(steps):
        x.requires_grad_(True)

        delta = (x - primary)    # shape: (B, D)
        logits = model(delta)    # shape: (B, 1)
        logits = logits.squeeze(-1)   # shape: (B)

        loss = criterion(logits, y_t)

        # gradient wrt x only (no model param grads)
        grad_x = torch.autograd.grad(loss, x, only_inputs=True, retain_graph=False, create_graph=False)[0]

        with torch.no_grad():
            x.add_(-alpha * torch.sign(grad_x))
            clamp_linf_(x, x0, epsilon)

        x.requires_grad_(False)

    return x.detach()   # shape: (B, D)
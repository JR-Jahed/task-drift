{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:27.716093Z",
     "start_time": "2025-08-01T14:52:27.714281Z"
    }
   },
   "source": "### Let's try to find a suffix that works on many prompts in attacking all the classifiers of Phi 3 model",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:31.017569Z",
     "start_time": "2025-08-01T14:52:27.832983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from custom_model import CustomModel\n",
    "from opt_utils import get_nonascii_toks, token_gradients, sample_control, get_filtered_cands, get_logits, load_model_and_tokenizer, get_prompt, get_primary_activation\n",
    "from suffix_manager import SuffixManager\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=10000)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=100000, threshold=float('inf'))"
   ],
   "id": "a76469392cccbdb4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40456997@eeecs.qub.ac.uk/PycharmProjects/TaskDriftTest/venv/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:31.028946Z",
     "start_time": "2025-08-01T14:52:31.027059Z"
    }
   },
   "cell_type": "code",
   "source": "model_name = 'phi3'",
   "id": "61dc82a34b75be41",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:31.081655Z",
     "start_time": "2025-08-01T14:52:31.079445Z"
    }
   },
   "cell_type": "code",
   "source": "model_path = f'./loaded_models/{model_name}'",
   "id": "fd75530eefeaa39",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:31.134711Z",
     "start_time": "2025-08-01T14:52:31.131806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adv_string_init = \"! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\"\n",
    "\n",
    "prompt_indices = random.sample(range(31134), 50)\n",
    "\n",
    "num_steps = 500\n",
    "topk = 64\n",
    "batch_size = 64\n",
    "allow_non_ascii = False  # you can set this to True to use unicode tokens"
   ],
   "id": "c362a33e70f50667",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:31.184684Z",
     "start_time": "2025-08-01T14:52:31.182317Z"
    }
   },
   "cell_type": "code",
   "source": "print(prompt_indices)",
   "id": "4d753354fe828ed5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20785, 24784, 6421, 16005, 30668, 27979, 18511, 17384, 4647, 241, 5449, 20857, 5127, 24595, 26786, 12442, 28924, 23910, 146, 5158, 24927, 18750, 26627, 20251, 30321, 27385, 22990, 17337, 1443, 27878, 14417, 22840, 7006, 22751, 19697, 10920, 25625, 23727, 9164, 21651, 16725, 1889, 8918, 22793, 14269, 20149, 25118, 6723, 18125, 4590]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:31.234182Z",
     "start_time": "2025-08-01T14:52:31.232086Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "860a3e9db39a6b11",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:32.039921Z",
     "start_time": "2025-08-01T14:52:31.336395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the text from the test dataset\n",
    "texts = [get_prompt(prompt_indices[0])]"
   ],
   "id": "bfe0149b5292e784",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:34.908349Z",
     "start_time": "2025-08-01T14:52:32.052498Z"
    }
   },
   "cell_type": "code",
   "source": "model, tokenizer = load_model_and_tokenizer(model_path)",
   "id": "78e07875f866f3c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8968af0fb0ba468a8a97a31e2958c1af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:34.934824Z",
     "start_time": "2025-08-01T14:52:34.932120Z"
    }
   },
   "cell_type": "code",
   "source": "suffix_manager = SuffixManager(tokenizer, texts[0], adv_string_init)",
   "id": "41e123675a6c3330",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:35.024083Z",
     "start_time": "2025-08-01T14:52:34.994688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear_models = {}\n",
    "\n",
    "layers = []\n",
    "\n",
    "for (dirpath, dir_names, filenames) in os.walk(f'./trained_linear_probes_microsoft/{model_name}'):\n",
    "    layers = [int(dir_name) for dir_name in dir_names]\n",
    "    break\n",
    "\n",
    "layers.sort()\n",
    "\n",
    "for i in layers:\n",
    "    linear_models[i] = pickle.load(open(f'./trained_linear_probes_microsoft/{model_name}/{i}/model.pickle', 'rb'))\n",
    "\n",
    "custom_model = CustomModel(model, linear_models)\n",
    "device = custom_model.base_model.get_input_embeddings().weight.device"
   ],
   "id": "d627bf4f1ed4c2da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40456997@eeecs.qub.ac.uk/PycharmProjects/TaskDriftTest/venv/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.4.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:35.058943Z",
     "start_time": "2025-08-01T14:52:35.056359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_primary_activations(prompt_index):\n",
    "\n",
    "    primary_activations = {\n",
    "        layer: get_primary_activation(index=prompt_index, model=model_name, layer=layer).to(device) for layer in layers\n",
    "    }\n",
    "\n",
    "    return primary_activations"
   ],
   "id": "ec12896288a91401",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:36.067607Z",
     "start_time": "2025-08-01T14:52:35.125227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A list of dict\n",
    "primary_activations_all_prompts = [get_primary_activations(prompt_indices[0])]"
   ],
   "id": "e7b09d6b81800291",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:36.268821Z",
     "start_time": "2025-08-01T14:52:36.266030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target = torch.tensor([0], dtype=torch.float32)\n",
    "target = target.to(device)"
   ],
   "id": "fe832b2f42ff4ea6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:36.468763Z",
     "start_time": "2025-08-01T14:52:36.463467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_misclassification(adv_suffix, suffix_manager, primary_activations):\n",
    "    device = custom_model.base_model.get_input_embeddings().weight.device\n",
    "    input_ids = suffix_manager.get_input_ids(adv_suffix).unsqueeze(0)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    for num_layer, linear_model in linear_models.items():\n",
    "        poisoned_activation = outputs['hidden_states'][num_layer + 1][0, -1].float()\n",
    "        delta = (poisoned_activation - primary_activations[num_layer]).detach().cpu().float().numpy().reshape(1, -1)\n",
    "\n",
    "        label = linear_model.predict(delta)\n",
    "        prob = linear_model.predict_proba(delta)\n",
    "\n",
    "        labels.append(label[0].tolist())\n",
    "        probs.append(prob[0].tolist())\n",
    "\n",
    "    return labels, probs\n",
    "\n"
   ],
   "id": "843446877523754",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:36.632258Z",
     "start_time": "2025-08-01T14:52:36.628879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_probs(probs):\n",
    "    formatted_probs = []\n",
    "    for prob_pair in probs:\n",
    "        formatted_pair = [f\"{p:.8f}\" for p in prob_pair]\n",
    "        formatted_probs.append(f\"[{formatted_pair[0]}, {formatted_pair[1]}]\")\n",
    "    probs_str = \"[\" + \", \".join(formatted_probs) + \"]\"\n",
    "\n",
    "    return probs_str\n",
    "\n",
    "def format_losses(losses):\n",
    "    losses_str = \"[\" + \", \".join([f'{loss:.8f}' for loss in losses]) + \"]\"\n",
    "    return losses_str"
   ],
   "id": "7b73452d99070e33",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:52:36.828667Z",
     "start_time": "2025-08-01T14:52:36.825697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_probabilities(probs_all_prompts_all_layers):\n",
    "    mn = 100.0\n",
    "\n",
    "    for probs_current_prompt_all_layers in probs_all_prompts_all_layers:\n",
    "        for probs_current_prompt_current_layer in probs_current_prompt_all_layers:\n",
    "            mn = min(mn, probs_current_prompt_current_layer[0])\n",
    "    return mn"
   ],
   "id": "d2ff3455d531b07c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer)\n",
    "adv_suffix = adv_string_init\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(num_steps + 1):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    coordinate_grad_all_prompts = None\n",
    "    losses_all_prompts_all_layers = []\n",
    "\n",
    "    input_ids = None\n",
    "\n",
    "    # A list of lists\n",
    "    labels_all_prompts_all_layers = []\n",
    "\n",
    "    # A list of lists of lists\n",
    "    probs_all_prompts_all_layers = []\n",
    "\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    for text, primary_activations_current_prompt in zip(texts, primary_activations_all_prompts):\n",
    "\n",
    "        # Step 1. Encode user prompt (behavior + adv suffix) as tokens and return token ids.\n",
    "        suffix_manager = SuffixManager(tokenizer, text, adv_string_init)\n",
    "        input_ids = suffix_manager.get_input_ids(adv_suffix)\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Step 2. Compute Coordinate Gradient\n",
    "        coordinate_grad_current_prompt, losses_current_prompt_all_layers, outputs, one_hot = token_gradients(custom_model, input_ids, suffix_manager.adv_string_slice,\n",
    "                                                                    target, primary_activations=primary_activations_current_prompt)\n",
    "\n",
    "        losses_all_prompts_all_layers.append(losses_current_prompt_all_layers)\n",
    "\n",
    "        if coordinate_grad_all_prompts is None:\n",
    "            coordinate_grad_all_prompts = coordinate_grad_current_prompt\n",
    "        else:\n",
    "            coordinate_grad_all_prompts += coordinate_grad_current_prompt\n",
    "\n",
    "        labels_current_prompt_all_layers, probs_current_prompt_all_layers = check_misclassification(adv_suffix, suffix_manager, primary_activations_current_prompt)\n",
    "        labels_all_prompts_all_layers.append(labels_current_prompt_all_layers)\n",
    "        probs_all_prompts_all_layers.append(probs_current_prompt_all_layers)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "    print(f\"i: {i}\")\n",
    "\n",
    "    for idx in range(len(labels_all_prompts_all_layers)):\n",
    "        print(f\"losses: {format_losses(losses_all_prompts_all_layers[idx])} labels: {labels_all_prompts_all_layers[idx]} probs: {format_probs(probs_all_prompts_all_layers[idx])}\")\n",
    "    print(adv_suffix)\n",
    "    print(\"-------------------------------------------------------\\n\")\n",
    "\n",
    "    if i == num_steps:\n",
    "        break\n",
    "\n",
    "    # Step 3. Sample a batch of new tokens based on the coordinate gradient.\n",
    "    # Notice that we only need the one that minimizes the loss.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Step 3.1 Slice the input to locate the adversarial suffix.\n",
    "        adv_suffix_tokens = input_ids[suffix_manager.adv_string_slice].to(device)\n",
    "\n",
    "        # Step 3.2 Randomly sample a batch of replacements.\n",
    "        # Encoded suffixes\n",
    "        new_adv_suffix_tokens = sample_control(\n",
    "            adv_suffix_tokens,\n",
    "            coordinate_grad_all_prompts,\n",
    "            batch_size,\n",
    "            topk=topk\n",
    "        )\n",
    "\n",
    "        # Step 3.3 This step ensures all adversarial candidates have the same number of tokens.\n",
    "        # Decoded suffixes\n",
    "        new_adv_suffix = get_filtered_cands(\n",
    "            tokenizer,\n",
    "            new_adv_suffix_tokens,\n",
    "            filter_cand=True,\n",
    "            curr_control=adv_suffix\n",
    "        )\n",
    "\n",
    "        losses_all_prompts_all_layers = None\n",
    "\n",
    "        for primary_activations_current_prompt in primary_activations_all_prompts:\n",
    "            # Step 3.4 Compute loss on these candidates and take the argmin.\n",
    "            logits_per_classifier = get_logits(\n",
    "                custom_model=custom_model,\n",
    "                tokenizer=tokenizer,\n",
    "                input_ids=input_ids,\n",
    "                control_slice=suffix_manager.adv_string_slice,\n",
    "                primary_activations=primary_activations_current_prompt,\n",
    "                test_controls=new_adv_suffix,\n",
    "                batch_size=8 # decrease this number if you run into OOM.\n",
    "            )\n",
    "            last_classifier_logits = next(reversed(logits_per_classifier.values()))\n",
    "            target = target.to(last_classifier_logits.dtype)\n",
    "            expanded_target = target.expand_as(last_classifier_logits)\n",
    "\n",
    "            losses_current_prompt_all_layers = None\n",
    "\n",
    "            for num_layer, logits_current_layer in logits_per_classifier.items():\n",
    "                losses_current_layer = nn.BCEWithLogitsLoss(reduction='none')(logits_current_layer, expanded_target)\n",
    "\n",
    "                if losses_current_prompt_all_layers is None:\n",
    "                    losses_current_prompt_all_layers = losses_current_layer\n",
    "                else:\n",
    "                    losses_current_prompt_all_layers += losses_current_layer\n",
    "                del logits_current_layer\n",
    "\n",
    "            if losses_all_prompts_all_layers is None:\n",
    "                losses_all_prompts_all_layers = losses_current_prompt_all_layers\n",
    "            else:\n",
    "                losses_all_prompts_all_layers += losses_current_prompt_all_layers\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        best_new_adv_suffix_id = losses_all_prompts_all_layers.argmin()\n",
    "        best_new_adv_suffix = new_adv_suffix[best_new_adv_suffix_id]\n",
    "\n",
    "        adv_suffix = best_new_adv_suffix\n",
    "\n",
    "    if len(texts) < len(prompt_indices) and check_probabilities(probs_all_prompts_all_layers) >= .8:\n",
    "        # Add next prompt\n",
    "        texts.append(get_prompt(prompt_indices[len(texts)]))\n",
    "\n",
    "        primary_activations = get_primary_activations(prompt_indices[len(primary_activations_all_prompts)])\n",
    "        # Add primary activations of this prompt\n",
    "        primary_activations_all_prompts.append(primary_activations)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time} seconds\")"
   ],
   "id": "97ac13ec1994bc4e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

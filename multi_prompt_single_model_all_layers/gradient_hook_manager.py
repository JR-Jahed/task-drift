import torch
import torch.nn as nn


class GradientHookManager:
    def __init__(self, model, pause_layers):
        self.model = model
        self.pause_layers = pause_layers
        self.hooks = []
        self.classifier_grads = {}

    def compute_classifier_gradients(self, logits, target, hidden_states):
        """
        Pre-compute gradients from each classifier w.r.t their respective hidden states
        CRITICAL: This must be called BEFORE registering hooks and BEFORE loss.backward()

        :param logits: A dict
        :param target: A tensor

        """
        loss_fn = nn.BCEWithLogitsLoss()

        print("Pre-computing classifier gradients...")

        for layer_idx in self.pause_layers:
            if layer_idx in logits:
                print(f"Computing classifier gradient for layer {layer_idx}")

                # Compute classifier loss
                classifier_loss = loss_fn(logits[layer_idx], target)

                # Compute gradient w.r.t the hidden state at this layer
                classifier_grad = torch.autograd.grad(
                    outputs=classifier_loss,
                    inputs=hidden_states[layer_idx],  # Use the actual hidden state tensor
                    retain_graph=True,
                    create_graph=False,
                    allow_unused=True
                )[0]

                if classifier_grad is not None:
                    self.classifier_grads[layer_idx] = classifier_grad.detach().clone()
                    print(f"Stored classifier gradient for layer {layer_idx} (norm: {classifier_grad.norm().item():.6f})")
                else:
                    print(f"No gradient computed for layer {layer_idx}")

        print(f"Pre-computed classifier gradients for layers: {list(self.classifier_grads.keys())}")

    def register_hooks(self, hidden_states):
        """Register backward hooks on hidden states at pause layers"""

        def create_hook(layer_idx):
            def hook_fn(grad):
                print(f"Hook called at layer {layer_idx} - incoming gradient norm: {grad.norm().item():.6f}")

                # Start with the incoming gradient (from higher layers)
                accumulated_grad = grad.clone()

                # Add pre-computed classifier gradient if available
                if layer_idx in self.classifier_grads:
                    print(f"Adding classifier gradient for layer {layer_idx}")
                    accumulated_grad += self.classifier_grads[layer_idx]
                    print(f"Accumulated gradient norm: {accumulated_grad.norm().item():.6f}")
                else:
                    print(f"No classifier gradient available for layer {layer_idx}")

                # Return accumulated gradient to continue backward flow
                return accumulated_grad

            return hook_fn

        # Register hooks on pause layers
        for layer_idx in self.pause_layers:
            if layer_idx < len(hidden_states):
                print(f"Registering hook on layer {layer_idx}")
                hook = hidden_states[layer_idx].register_hook(create_hook(layer_idx))
                self.hooks.append(hook)

    def remove_hooks(self):
        """Remove all registered hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
        print("All hooks removed")

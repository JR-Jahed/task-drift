{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:17.573066Z",
     "start_time": "2025-08-19T15:40:17.571444Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:20.968530Z",
     "start_time": "2025-08-19T15:40:17.723969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from custom_model import CustomModel\n",
    "from opt_utils import get_nonascii_toks, token_gradients, sample_control, get_filtered_cands, get_logits, load_model_and_tokenizer, get_training_prompts, get_primary_activation\n",
    "from llama.suffix_manager import SuffixManager\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from constants import PROJECT_ROOT\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=10000)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=100000, threshold=float('inf'))"
   ],
   "id": "2dd6331876de19fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40456997@eeecs.qub.ac.uk/PycharmProjects/TaskDriftTest/venv/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:20.990446Z",
     "start_time": "2025-08-19T15:40:20.987781Z"
    }
   },
   "cell_type": "code",
   "source": "model_name = 'llama3_8b'",
   "id": "5b8b1623ee129814",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:21.031434Z",
     "start_time": "2025-08-19T15:40:21.028665Z"
    }
   },
   "cell_type": "code",
   "source": "model_path = f'{PROJECT_ROOT}/loaded_models/{model_name}'",
   "id": "fd75530eefeaa39",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:21.080393Z",
     "start_time": "2025-08-19T15:40:21.077972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adv_string_init = \"!_!_!_!_!_!_!_!_!_!_!\"\n",
    "\n",
    "# Randomly select 100 prompts from the range [100000, 200000)\n",
    "prompt_indices = random.sample(range(100000, 200000), 100)\n",
    "\n",
    "num_steps = 1000\n",
    "topk = 256\n",
    "batch_size = 512\n",
    "allow_non_ascii = False  # you can set this to True to use unicode tokens"
   ],
   "id": "c362a33e70f50667",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:21.134090Z",
     "start_time": "2025-08-19T15:40:21.131148Z"
    }
   },
   "cell_type": "code",
   "source": "print(prompt_indices)",
   "id": "4d753354fe828ed5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134647, 102465, 180935, 100433, 116899, 177111, 111514, 102729, 167388, 154555, 114416, 100348, 184556, 157426, 197814, 151497, 165879, 148250, 116148, 105555, 159466, 118739, 109657, 143332, 140662, 126467, 160877, 146908, 110580, 110567, 134504, 169460, 133795, 139652, 187935, 157386, 178011, 117249, 129219, 139864, 115083, 131958, 174868, 151816, 100824, 135764, 118689, 153469, 162062, 184190, 165309, 116221, 195402, 178066, 199337, 147122, 140310, 166235, 159496, 109225, 175462, 137992, 129429, 168266, 162105, 137192, 129747, 122774, 184744, 169442, 116167, 149081, 165979, 193931, 159433, 136857, 132361, 188252, 186556, 146983, 140214, 172633, 113923, 158724, 136408, 144437, 162219, 127329, 154591, 132327, 138477, 167611, 147988, 137177, 191591, 125640, 104214, 105192, 122292, 155786]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:40:21.189172Z",
     "start_time": "2025-08-19T15:40:21.186985Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "860a3e9db39a6b11",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-19T15:40:21.241487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the text from the test dataset\n",
    "training_prompts = get_training_prompts()\n",
    "\n",
    "texts = [training_prompts[prompt_indices[0]]]"
   ],
   "id": "bfe0149b5292e784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:05.014263Z",
     "start_time": "2025-08-19T15:37:49.143735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "print(model.dtype)\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "model32 = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, device_map='cpu').eval()\n",
    "print(model32.dtype)\n"
   ],
   "id": "78e07875f866f3c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55088b6e47174661938d1f83791f51be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5331e8e36fb457aa6f0b23974fb93d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:05.275833Z",
     "start_time": "2025-08-19T15:38:05.225086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear_models = {}\n",
    "\n",
    "layers = []\n",
    "\n",
    "for (dirpath, dir_names, filenames) in os.walk(f'{PROJECT_ROOT}/trained_linear_probes_microsoft/{model_name}'):\n",
    "    layers = [int(dir_name) for dir_name in dir_names]\n",
    "    break\n",
    "\n",
    "layers.sort()\n",
    "\n",
    "for i in layers:\n",
    "    linear_models[i] = pickle.load(open(f'{PROJECT_ROOT}/trained_linear_probes_microsoft/{model_name}/{i}/model.pickle', 'rb'))\n",
    "\n",
    "custom_model = CustomModel(model, linear_models)\n",
    "device = custom_model.base_model.get_input_embeddings().weight.device"
   ],
   "id": "d627bf4f1ed4c2da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40456997@eeecs.qub.ac.uk/PycharmProjects/TaskDriftTest/venv/lib/python3.10/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.4.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:05.358233Z",
     "start_time": "2025-08-19T15:38:05.355152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_primary_activations(prompt_index):\n",
    "\n",
    "    primary_activations = {\n",
    "        layer: get_primary_activation(index=prompt_index, model=model_name, layer=layer, subset='training').to(device) for layer in layers\n",
    "    }\n",
    "\n",
    "    return primary_activations"
   ],
   "id": "ec12896288a91401",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:09.492812Z",
     "start_time": "2025-08-19T15:38:05.476055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A list of dict\n",
    "primary_activations_all_prompts = [get_primary_activations(prompt_indices[0])]"
   ],
   "id": "e7b09d6b81800291",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:09.884453Z",
     "start_time": "2025-08-19T15:38:09.881836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target = torch.tensor([0], dtype=torch.float32)\n",
    "target = target.to(device)"
   ],
   "id": "fe832b2f42ff4ea6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:10.167709Z",
     "start_time": "2025-08-19T15:38:10.162710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_misclassification(adv_suffix, suffix_manager, primary_activations):\n",
    "    device = custom_model.base_model.get_input_embeddings().weight.device\n",
    "    input_ids = suffix_manager.get_input_ids(adv_suffix).unsqueeze(0)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    # outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    outputs = model32(input_ids=input_ids.to('cpu'), attention_mask=attention_mask.to('cpu'), output_hidden_states=True)\n",
    "\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    for num_layer, linear_model in linear_models.items():\n",
    "        poisoned_activation = outputs['hidden_states'][num_layer + 1][0, -1].float()\n",
    "        # delta = (poisoned_activation - primary_activations[num_layer]).detach().cpu().float().numpy().reshape(1, -1)   # gpu version\n",
    "        delta = (poisoned_activation - primary_activations[num_layer].to('cpu')).detach().cpu().float().numpy().reshape(1, -1)  # cpu version\n",
    "\n",
    "        label = linear_model.predict(delta)\n",
    "        prob = linear_model.predict_proba(delta)\n",
    "\n",
    "        labels.append(label[0].tolist())\n",
    "        probs.append(prob[0].tolist())\n",
    "\n",
    "    return labels, probs\n"
   ],
   "id": "843446877523754",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:10.494614Z",
     "start_time": "2025-08-19T15:38:10.491169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_probs(probs):\n",
    "    formatted_probs = []\n",
    "    for prob_pair in probs:\n",
    "        formatted_pair = [f\"{p:.8f}\" for p in prob_pair]\n",
    "        formatted_probs.append(f\"[{formatted_pair[0]}, {formatted_pair[1]}]\")\n",
    "    probs_str = \"[\" + \", \".join(formatted_probs) + \"]\"\n",
    "\n",
    "    return probs_str\n",
    "\n",
    "def format_losses(losses):\n",
    "    losses_str = \"[\" + \", \".join([f'{loss:.8f}' for loss in losses]) + \"]\"\n",
    "    return losses_str"
   ],
   "id": "7b73452d99070e33",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:38:10.782218Z",
     "start_time": "2025-08-19T15:38:10.778088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_probabilities(probs_all_prompts_all_layers):\n",
    "    mn = 100.0\n",
    "\n",
    "    for probs_current_prompt_all_layers in probs_all_prompts_all_layers:\n",
    "        for probs_current_prompt_current_layer in probs_current_prompt_all_layers:\n",
    "            mn = min(mn, probs_current_prompt_current_layer[0])\n",
    "    return mn\n",
    "\n",
    "\n",
    "def percentage_of_successful_prompts_all_layers(probs_all_prompts_all_layers, confidence_threshold=.8):\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for probs_current_prompt_all_layers in probs_all_prompts_all_layers:\n",
    "        mn = 100.0\n",
    "        for probs_current_prompt_current_layer in probs_current_prompt_all_layers:\n",
    "            mn = min(mn, probs_current_prompt_current_layer[0])\n",
    "\n",
    "        if mn >= confidence_threshold:\n",
    "            cnt += 1\n",
    "\n",
    "    return cnt / len(probs_all_prompts_all_layers)\n",
    "\n",
    "\n",
    "def add_next_prompt():\n",
    "    # Add next prompt\n",
    "    texts.append(training_prompts[prompt_indices[len(texts)]])\n",
    "\n",
    "    primary_activations = get_primary_activations(prompt_indices[len(primary_activations_all_prompts)])\n",
    "    # Add primary activations of this prompt\n",
    "    primary_activations_all_prompts.append(primary_activations)\n"
   ],
   "id": "d2ff3455d531b07c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-08-19T15:39:58.201978330Z",
     "start_time": "2025-08-19T15:38:11.100833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "suffix_manager = SuffixManager(tokenizer, texts[0], adv_string_init)\n",
    "\n",
    "not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer)\n",
    "\n",
    "adv_suffix = adv_string_init\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "last_added = 0\n",
    "\n",
    "for i in range(0, num_steps + 1):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    coordinate_grad_all_prompts = None\n",
    "    losses_all_prompts_all_layers = []\n",
    "\n",
    "    input_ids = None\n",
    "\n",
    "    # A list of lists\n",
    "    labels_all_prompts_all_layers = []\n",
    "\n",
    "    # A list of lists of lists\n",
    "    probs_all_prompts_all_layers = []\n",
    "\n",
    "    for text, primary_activations_current_prompt in zip(texts, primary_activations_all_prompts):\n",
    "\n",
    "        # Step 1. Encode user prompt (behavior + adv suffix) as tokens and return token ids.\n",
    "        suffix_manager = SuffixManager(tokenizer, text, adv_string_init)\n",
    "\n",
    "        input_ids = suffix_manager.get_input_ids(adv_suffix)\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Step 2. Compute Coordinate Gradient\n",
    "        coordinate_grad_current_prompt, losses_current_prompt_all_layers = token_gradients(custom_model, input_ids, suffix_manager.adv_string_slice,\n",
    "                                                                    target, primary_activations=primary_activations_current_prompt)\n",
    "\n",
    "        losses_all_prompts_all_layers.append(losses_current_prompt_all_layers)\n",
    "\n",
    "        if coordinate_grad_all_prompts is None:\n",
    "            coordinate_grad_all_prompts = coordinate_grad_current_prompt\n",
    "        else:\n",
    "            coordinate_grad_all_prompts += coordinate_grad_current_prompt\n",
    "\n",
    "        labels_current_prompt_all_layers, probs_current_prompt_all_layers = check_misclassification(adv_suffix, suffix_manager, primary_activations_current_prompt)\n",
    "        labels_all_prompts_all_layers.append(labels_current_prompt_all_layers)\n",
    "        probs_all_prompts_all_layers.append(probs_current_prompt_all_layers)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"i: {i}\")\n",
    "\n",
    "    for idx in range(len(labels_all_prompts_all_layers)):\n",
    "        print(f\"losses: {format_losses(losses_all_prompts_all_layers[idx])} labels: {labels_all_prompts_all_layers[idx]} probs: {format_probs(probs_all_prompts_all_layers[idx])}\")\n",
    "    print(f\"{adv_suffix}\")\n",
    "    print(\"-------------------------------------------------------\\n\")\n",
    "\n",
    "    if i == num_steps:\n",
    "        break\n",
    "\n",
    "    # Step 3. Sample a batch of new tokens based on the coordinate gradient.\n",
    "    # Notice that we only need the one that minimizes the loss.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Step 3.1 Slice the input to locate the adversarial suffix.\n",
    "        adv_suffix_tokens = input_ids[suffix_manager.adv_string_slice].to(device)\n",
    "        # print(f\"adv suffix: {adv_suffix_tokens}   {adv_suffix_tokens.shape}    {[tokenizer.decode(tk.item()) for tk in adv_suffix_tokens]}   after suffix manager\")\n",
    "\n",
    "        # Step 3.2 Select all the possible suffixes after replacements\n",
    "        # Many suffixes — sometimes all — are filtered in the function get_filtered_cands due to length mismatch\n",
    "        # after decoding and re-encoding. Therefore, we replace each position with topk tokens in sample_control\n",
    "        # and randomly select batch_size after filtering\n",
    "\n",
    "        # Encoded suffixes\n",
    "        new_adv_suffix_tokens = sample_control(\n",
    "            adv_suffix_tokens,\n",
    "            coordinate_grad_all_prompts,\n",
    "            topk=topk,\n",
    "            not_allowed_tokens=not_allowed_tokens\n",
    "        )\n",
    "\n",
    "        # Step 3.3 This step ensures all adversarial candidates have the same number of tokens.\n",
    "        # Decoded suffixes\n",
    "        new_adv_suffix = get_filtered_cands(\n",
    "            tokenizer,\n",
    "            new_adv_suffix_tokens,\n",
    "            new_batch_size=batch_size,\n",
    "            filter_cand=True,\n",
    "            curr_control=adv_suffix\n",
    "        )\n",
    "\n",
    "        losses_all_prompts_all_layers = None\n",
    "\n",
    "        for text, primary_activations_current_prompt in zip(texts, primary_activations_all_prompts):\n",
    "\n",
    "            suffix_manager = SuffixManager(tokenizer, text, adv_string_init)\n",
    "            input_ids = suffix_manager.get_input_ids(adv_suffix)\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            # Step 3.4 Compute loss on these candidates and take the argmin.\n",
    "            logits_per_classifier = get_logits(\n",
    "                custom_model=custom_model,\n",
    "                tokenizer=tokenizer,\n",
    "                input_ids=input_ids,\n",
    "                control_slice=suffix_manager.adv_string_slice,\n",
    "                primary_activations=primary_activations_current_prompt,\n",
    "                test_controls=new_adv_suffix,\n",
    "                batch_size=8 # decrease this number if you run into OOM.\n",
    "            )\n",
    "            last_classifier_logits = next(reversed(logits_per_classifier.values()))\n",
    "            target = target.to(last_classifier_logits.dtype)\n",
    "            expanded_target = target.expand_as(last_classifier_logits)\n",
    "            expanded_target = expanded_target.to(last_classifier_logits.device)\n",
    "\n",
    "            losses_current_prompt_all_layers = None\n",
    "\n",
    "            for num_layer, logits_current_layer in logits_per_classifier.items():\n",
    "                losses_current_layer = nn.BCEWithLogitsLoss(reduction='none')(logits_current_layer, expanded_target)\n",
    "\n",
    "                if losses_current_prompt_all_layers is None:\n",
    "                    losses_current_prompt_all_layers = losses_current_layer\n",
    "                else:\n",
    "                    losses_current_prompt_all_layers += losses_current_layer\n",
    "                del logits_current_layer\n",
    "\n",
    "            if losses_all_prompts_all_layers is None:\n",
    "                losses_all_prompts_all_layers = losses_current_prompt_all_layers\n",
    "            else:\n",
    "                losses_all_prompts_all_layers += losses_current_prompt_all_layers\n",
    "            gc.collect()\n",
    "\n",
    "        best_new_adv_suffix_id = losses_all_prompts_all_layers.argmin()\n",
    "        best_new_adv_suffix = new_adv_suffix[best_new_adv_suffix_id]\n",
    "\n",
    "        adv_suffix = best_new_adv_suffix\n",
    "\n",
    "    if len(texts) < len(prompt_indices):\n",
    "\n",
    "        if percentage_of_successful_prompts_all_layers(probs_all_prompts_all_layers, confidence_threshold=.6) >= .8:\n",
    "            # If the attack is successful on 80% or more prompts, add next prompt\n",
    "            add_next_prompt()\n",
    "            last_added = i\n",
    "\n",
    "        if len(texts) >= 5 and i - last_added == 10:\n",
    "            # If there are already 5 or more prompts and the latest one or any previous one is being recalcitrant\n",
    "            # and the suffix is not working on it, it might be helpful to add a new prompt\n",
    "            add_next_prompt()\n",
    "            last_added = i\n",
    "        elif len(texts) >= 3 and i - last_added == 20:\n",
    "            add_next_prompt()\n",
    "            last_added = i\n",
    "        elif len(texts) == 2 and i - last_added == 30:\n",
    "            add_next_prompt()\n",
    "            last_added = i\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time} seconds\")"
   ],
   "id": "97ac13ec1994bc4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "losses: [5.28716040, 12.39862347, 24.60106277, 37.55773544, 79.33953857] labels: [1, 1, 1, 1, 1] probs: [[0.00480019, 0.99519981], [0.00000340, 0.99999660], [0.00000000, 1.00000000], [0.00000000, 1.00000000], [0.00000000, 1.00000000]]\n",
      "!_!_!_!_!_!_!_!_!_!_!\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40456997@eeecs.qub.ac.uk/PycharmProjects/TaskDriftTest/venv/lib/python3.10/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1\n",
      "losses: [3.56855440, 13.89861679, 24.98073959, 33.36594009, 73.01364899] labels: [1, 1, 1, 1, 1] probs: [[0.02731973, 0.97268027], [0.00000079, 0.99999921], [0.00000000, 1.00000000], [0.00000000, 1.00000000], [0.00000000, 1.00000000]]\n",
      "!_!_!_!_!_!_!_!_!_! ${({!\n",
      "-------------------------------------------------------\n",
      "\n",
      "i: 2\n",
      "losses: [3.23725510, 13.51581383, 20.81251335, 26.10292435, 56.99132919] labels: [1, 1, 1, 1, 1] probs: [[0.03957385, 0.96042615], [0.00000131, 0.99999869], [0.00000000, 1.00000000], [0.00000000, 1.00000000], [0.00000000, 1.00000000]]\n",
      "!_!_!_!_!_!_!_!_!_[${ ${({!\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

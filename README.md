# Task Drift Detection in LLMs

### Adversarial Attacks and Lightweight Defences

This repository contains the code and experiments from my MSc thesis, which studies the **adversarial robustness of task drift detection mechanisms in Large Language Models (LLMs)**.

Task drift detection, introduced by Microsoft, uses **lightweight linear probes (logistic regression classifiers) trained on hidden activations** to detect stealthy prompt injection attacks. While effective on clean data, the robustness of these probes under **adaptive, gradient-based attacks** remains poorly understood.

This work provides a systematic study of **attacks and defences for task drift detection**, focusing on suffix-based adversarial prompt injection.

---

### Core Findings

##### Key vulnerability
Task drift detection via linear probes is **highly vulnerable to universal adversarial suffixes**, even when probes are attached across multiple layers of the model.

##### Key defence
A simple and lightweight **randomised adversarial suffix training scheme** dramatically improves robustness (80â€“99%) while preserving clean accuracy, and proves **highly resistant to direct suffix optimisation**.

##### Implication
Robust task drift detection can be achieved **without modifying the base LLM**, using only probe-level defences with minimal computational overhead.

---

### Contributions
##### 1. Adversarial Attacks on Task Drift Detection

* Adapted **Greedy Coordinate Gradient (GCG)** for direct optimisation against task-drift probes using LLM activation gradients.

* Formulated **multi-probe attacks**, jointly optimising adversarial suffixes against classifiers attached to **multiple layers** of the model.

* Designed a **layer-wise gradient accumulation scheme** that combines classifier-specific losses with upstream backpropagation through the LLM.

* Discovered **universal adversarial suffixes** that generalise across prompts and consistently evade detection.

* Achieved **near-perfect attack success** against Microsoft-trained probes on:

  * **Phi-3 Mini (3.8B)**

  * **LLaMA-3 (8B)**

##### 2. Defence Mechanisms

* Evaluated **adversarial training with PGD**, finding it ineffective and highly unstable against suffix-based attacks.

* Proposed a new **randomised defence strategy**:

  1. Generate a diverse pool of adversarial suffixes.

  2. Randomly append one suffix to each prompt during probe training.

  3. Retrain classifiers on the resulting poisoned activations.

* This approach:

  * Preserves baseline accuracy on clean inputs.

  * Achieves **80â€“99% robustness** under strict evaluation.

  * Achieves **96â€“100% robustness** under majority-vote metrics.

* Direct suffix optimisation against defended classifiers proved **extremely difficult**, demonstrating strong empirical resilience.

---

### Method Overview

##### Multi-Probe Adversarial Optimisation

Adversarial suffixes are optimised by **backpropagating gradients from multiple linear probes attached to different layers** of the LLM. Each probe contributes a classifier-specific loss, which is accumulated and propagated upstream to guide suffix updates.

>This allows the attack to simultaneously evade detection across the entire probe stack, rather than overfitting to a single layer.

ðŸ“Œ **Figure below** illustrates the layer-wise gradient flow and accumulation mechanism used during backpropagation.

![Layer-wise Gradient Accumulation](Gradient%20Computation.png)

---

### Experimental Setup

* **Models**
  * [Phi-3 Mini (3.8B)](https://huggingface.co/microsoft/phi-3-mini-3.8b)
  * [LLaMA-3 (8B)](https://huggingface.co/meta-llama/Meta-Llama-3-8B)

* **Probes**
  * Logistic regression classifiers trained on hidden activations

* **Libraries**
  * PyTorch
  * HuggingFace Transformers

* **Techniques**
  * Gradient-based adversarial optimisation
  * Model probing
  * Adversarial training

---

### Results Summary

* **Attacks**

  * Universal suffixes consistently evade all probes across layers.
  * High transferability across prompts and model instances.

* **Defences**

  * Randomised suffix training significantly improves robustness with minimal complexity.
  * Strong resistance to adaptive suffix optimisation.

---

### Takeaway

>**Randomised adversarial suffix training is a simple, effective, and lightweight defence against prompt injection attacks targeting task drift detection systems.**

This work highlights both the **fragility of probe-based detection under adaptive attacks** and a practical path toward **robust, deployment-friendly defences**.
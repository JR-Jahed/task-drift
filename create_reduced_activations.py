import torch
import os
from utils.load_file_paths import load_file_paths
from constants import PROJECT_ROOT, ROOT_DIR_TRAIN, ROOT_DIR_VAL, ROOT_DIR_TEST


"""
Since we need only 5 LLM layers for this project, we can reduce
the size of the dataset by storing only the necessary layers
"""


model_name = 'llama3_8b'

activation_dir = ROOT_DIR_TRAIN[model_name]
reduced_activation_dir = activation_dir.replace('Activation', 'Reduced Activation')

os.makedirs(reduced_activation_dir, exist_ok=True)

file_paths = load_file_paths(f'{PROJECT_ROOT}/data_files/train_files_{model_name}.txt')

for i, file_path in enumerate(file_paths):
    activation_filepath = f'{activation_dir}/{file_path}'
    reduced_activation_filepath = activation_filepath.replace('Activation', 'Reduced Activation')

    if os.path.exists(activation_filepath):
        print(f"{i}   ----   Moving   {file_path}")
        activations = torch.load(activation_filepath)
        reduced_activations = activations[:, :, [0, 7, 15, 23, 31], :]

        # print(activations.shape)
        # print(reduced_activations.shape)
        # continue

        torch.save(reduced_activations, reduced_activation_filepath)
        os.remove(activation_filepath)

